{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dff8abe",
   "metadata": {},
   "source": [
    "# Router Agent Benchmark\n",
    "This notebook implements a benchmark using groundtruthing technique to evaluate AI models that function as **Router Agents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237fc70",
   "metadata": {},
   "source": [
    "## Setup LLM/SLM Provider\n",
    "In this example I'll use LangChain and OpenAI as LLM provider, switch to your provider (AWS Bedrock with boto3, Groq directly calls, etc.)\n",
    "\n",
    "To guarantee only the three words as result, I'll define an enum and use it as output parser (If the model answers something different from it, it will retry automatically.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "024f3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class CategoryEnum(str, Enum):\n",
    "    MATH = \"math\"\n",
    "    HISTORY = \"history\"\n",
    "    JOKE = \"joke\"\n",
    "\n",
    "class CategoryModel(BaseModel):\n",
    "    category_result: CategoryEnum = Field(description=\"The agent's category result\")\n",
    "\n",
    "#TODO: should bea function and receive the model as parameter\n",
    "parser = PydanticOutputParser(pydantic_object=CategoryModel)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "        You are a Router Agent specialized in classifying user questions into specific categories.\n",
    "        Your function is to analyze the user's question and determine which category best applies:\n",
    "            - **math**: Questions related to mathematical calculations, arithmetic operations, algebra, geometry, statistics, or any problem that requires mathematical reasoning or numerical computation.\n",
    "            - **history**: Questions about historical events, historical figures, important dates, historical periods, wars, revolutions, or any topic related to the past and history.\n",
    "            - **joke**: Requests to tell jokes, make humor, or any request related to humorous entertainment, regardless of language.\n",
    "        \n",
    "        **Important instructions:**\n",
    "            1. Carefully analyze the intention and content of the question\n",
    "            2. Identify the most appropriate category based on the main purpose of the question\n",
    "            3. Return EXCLUSIVELY the category in the specified format, without additional text or explanations\n",
    "            4. Be accurate and consistent in classification\n",
    "\n",
    "        Expected output format:\n",
    "        {format_instructions}\n",
    "    \"\"\"),\n",
    "    (\"human\", \"\"\"\n",
    "        Classify the following question into one of the available categories (math, history, or joke):\n",
    "        Question: {input}\"\"\")\n",
    "    ])\n",
    "\n",
    "def call_prompt(model_name: str, input: str):\n",
    "    model = ChatOpenAI(\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "    )\n",
    "    \n",
    "    chain = prompt | model | parser\n",
    "    \n",
    "    return chain.invoke({\n",
    "        \"input\": input,\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d83ac1",
   "metadata": {},
   "source": [
    "## Prepare Groundtruth Data\n",
    "Loads groundtruth data from JSON file to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7735774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "groundtruth_file_path = \"groundtruth.json\"\n",
    "\n",
    "with open(groundtruth_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    groundtruth_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7f4757",
   "metadata": {},
   "source": [
    "## Define Models to Test\n",
    "\n",
    "Define a list of models to test (should follow the provider names definition, in my case, OpenAI.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7c62c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_test = [\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-5.2\",\n",
    "    \"gpt-5-mini\",\n",
    "    \"gpt-5-nano\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306071c4",
   "metadata": {},
   "source": [
    "## Call Models and Collect Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6d82a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gpt-4o model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:49<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gpt-4o-mini model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:42<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gpt-5.2 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:07<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gpt-5-mini model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:59<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gpt-5-nano model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [03:31<00:00,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                    BENCHMARK RESULTS\n",
      "======================================================================\n",
      "\n",
      "┌────────────────────────────────────────────────────────────────────┐\n",
      "│ Model: gpt-4o                                                      │\n",
      "├────────────────────────────────────────────────────────────────────┤\n",
      "│ Success Rate:    100.00%                                           │\n",
      "│ Latency Average:   710.78 ms                                       │\n",
      "│ Latency P50:       632.00 ms                                       │\n",
      "│ Latency P90:       999.47 ms                                       │\n",
      "│ Latency P95:      1076.29 ms                                       │\n",
      "└────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "┌────────────────────────────────────────────────────────────────────┐\n",
      "│ Model: gpt-4o-mini                                                 │\n",
      "├────────────────────────────────────────────────────────────────────┤\n",
      "│ Success Rate:    100.00%                                           │\n",
      "│ Latency Average:   619.55 ms                                       │\n",
      "│ Latency P50:       598.05 ms                                       │\n",
      "│ Latency P90:       733.47 ms                                       │\n",
      "│ Latency P95:       770.13 ms                                       │\n",
      "└────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "┌────────────────────────────────────────────────────────────────────┐\n",
      "│ Model: gpt-5.2                                                     │\n",
      "├────────────────────────────────────────────────────────────────────┤\n",
      "│ Success Rate:    100.00%                                           │\n",
      "│ Latency Average:   978.90 ms                                       │\n",
      "│ Latency P50:       923.60 ms                                       │\n",
      "│ Latency P90:      1417.46 ms                                       │\n",
      "│ Latency P95:      1448.01 ms                                       │\n",
      "└────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "┌────────────────────────────────────────────────────────────────────┐\n",
      "│ Model: gpt-5-mini                                                  │\n",
      "├────────────────────────────────────────────────────────────────────┤\n",
      "│ Success Rate:    100.00%                                           │\n",
      "│ Latency Average:  1730.41 ms                                       │\n",
      "│ Latency P50:      1593.34 ms                                       │\n",
      "│ Latency P90:      2251.54 ms                                       │\n",
      "│ Latency P95:      2610.01 ms                                       │\n",
      "└────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "┌────────────────────────────────────────────────────────────────────┐\n",
      "│ Model: gpt-5-nano                                                  │\n",
      "├────────────────────────────────────────────────────────────────────┤\n",
      "│ Success Rate:    100.00%                                           │\n",
      "│ Latency Average:  3070.03 ms                                       │\n",
      "│ Latency P50:      2912.30 ms                                       │\n",
      "│ Latency P90:      3819.75 ms                                       │\n",
      "│ Latency P95:      4133.14 ms                                       │\n",
      "└────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class ModelResult(BaseModel):\n",
    "    model_name: str\n",
    "    success_rate: float\n",
    "    latency_p50: float\n",
    "    latency_p90: float\n",
    "    latency_p95: float\n",
    "    latency_average: float\n",
    "\n",
    "model_results = []\n",
    "all_dataframes = []\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"Testing {model} model\")\n",
    "    inputs = []\n",
    "    expected_outputs = []\n",
    "    model_outputs = []\n",
    "    is_matches = []\n",
    "    latencies = []\n",
    "\n",
    "    \n",
    "    for case in tqdm(groundtruth_data):\n",
    "        inputs.append(case[\"input\"])\n",
    "        expected_outputs.append(case[\"expected_output\"])\n",
    "        \n",
    "        start = time.time()\n",
    "        result = call_prompt(model, case[\"input\"])\n",
    "        end = time.time()\n",
    "\n",
    "        latency = end - start\n",
    "        latencies.append(latency)\n",
    "\n",
    "        model_output_str = str(result.category_result.value)\n",
    "        model_outputs.append(model_output_str)\n",
    "\n",
    "        if model_output_str == case[\"expected_output\"]:\n",
    "            is_matches.append(True)\n",
    "        else:\n",
    "            is_matches.append(False)\n",
    "\n",
    "    min_latency = min(latencies)\n",
    "    max_latency = max(latencies)\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Input\": inputs,\n",
    "        \"Expected output\": expected_outputs,\n",
    "        \"Model output\": model_outputs,\n",
    "        \"Is correct\": is_matches\n",
    "    })\n",
    "\n",
    "    model_results.append(\n",
    "        ModelResult(\n",
    "            model_name=model,\n",
    "            success_rate=df['Is correct'].mean() * 100,\n",
    "            latency_p50=np.percentile(latencies, 50),\n",
    "            latency_p90=np.percentile(latencies, 90),\n",
    "            latency_p95=np.percentile(latencies, 95),\n",
    "            latency_average=sum(latencies) / len(latencies)\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \" * 20 + \"BENCHMARK RESULTS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for model in model_results:\n",
    "    print(f\"┌{'─'*68}┐\")\n",
    "    print(f\"│ Model: {model.model_name:<59} │\")\n",
    "    print(f\"├{'─'*68}┤\")\n",
    "    \n",
    "    success_str = f\"Success Rate:    {model.success_rate:>6.2f}%\"\n",
    "    avg_str = f\"Latency Average: {model.latency_average * 1000:>8.2f} ms\"\n",
    "    p50_str = f\"Latency P50:     {model.latency_p50 * 1000:>8.2f} ms\"\n",
    "    p90_str = f\"Latency P90:     {model.latency_p90 * 1000:>8.2f} ms\"\n",
    "    p95_str = f\"Latency P95:     {model.latency_p95 * 1000:>8.2f} ms\"\n",
    "    \n",
    "    print(f\"│ {success_str:<66} │\")\n",
    "    print(f\"│ {avg_str:<66} │\")\n",
    "    print(f\"│ {p50_str:<66} │\")\n",
    "    print(f\"│ {p90_str:<66} │\")\n",
    "    print(f\"│ {p95_str:<66} │\")\n",
    "    print(f\"└{'─'*68}┘\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
